{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interface import *\n",
    "import numpy as np\n",
    "\n",
    "# ================================ 2.1.1 ReLU ================================\n",
    "class ReLU(Layer):\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: np.array((n, ...)), input values,\n",
    "                n - batch size, ... - arbitrary input shape\n",
    "        :return: np.array((n, ...)), output values,\n",
    "                n - batch size, ... - arbitrary output shape (same as input)\n",
    "        \"\"\"\n",
    "        \n",
    "        return inputs * (inputs > 0)\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        :param grad_outputs: np.array((n, ...)), dLoss/dOutputs,\n",
    "                n - batch size, ... - arbitrary output shape\n",
    "        :return: np.array((n, ...)), dLoss/dInputs,\n",
    "                n - batch size, ... - arbitrary input shape (same as output)\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        inputs = self.forward_inputs\n",
    "        return np.ones(grad_outputs.shape) * (inputs > 0) * grad_outputs\n",
    "        # your code here /\\\n",
    "\n",
    "\n",
    "# ============================== 2.1.2 Softmax ===============================\n",
    "class Softmax(Layer):\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: np.array((n, d)), input values,\n",
    "                n - batch size, d - number of units\n",
    "        :return: np.array((n, d)), output values,\n",
    "                n - batch size, d - number of units\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        result = np.exp(inputs)\n",
    "        return (result.T / np.sum(result, axis=1)).T\n",
    "        # your code here /\\\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        :param grad_outputs: np.array((n, d)), dLoss/dOutputs,\n",
    "                n - batch size, d - number of units\n",
    "        :return: np.array((n, d)), dLoss/dInputs,\n",
    "                n - batch size, d - number of units\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        answer = np.zeros_like(grad_outputs)\n",
    "        outputs = self.forward_outputs\n",
    "        for ans, out, grad_out in zip(answer, outputs, grad_outputs):\n",
    "            ans[:] = (np.diag(out) - np.matrix(out).T @ np.matrix(out)) @ grad_out\n",
    "        return answer\n",
    "        # your code here /\\\n",
    "\n",
    "\n",
    "# =============================== 2.1.3 Dense ================================\n",
    "class Dense(Layer):\n",
    "    def __init__(self, units, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.output_shape = (units,)\n",
    "        self.weights, self.weights_grad = None, None\n",
    "        self.biases, self.biases_grad = None, None\n",
    "\n",
    "    def build(self, *args, **kwargs):\n",
    "        super().build(*args, **kwargs)\n",
    "\n",
    "        input_units, = self.input_shape\n",
    "        output_units, = self.output_shape\n",
    "\n",
    "        # Register weights and biases as trainable parameters\n",
    "        # Note, that the parameters and gradients *must* be stored in\n",
    "        # self.<p> and self.<p>_grad, where <p> is the name specified in\n",
    "        # self.add_parameter\n",
    "\n",
    "        self.weights, self.weights_grad = self.add_parameter(\n",
    "            name='weights',\n",
    "            shape=(input_units, output_units),\n",
    "            initializer=he_initializer(input_units)\n",
    "        )\n",
    "\n",
    "        self.biases, self.biases_grad = self.add_parameter(\n",
    "            name='biases',\n",
    "            shape=(output_units,),\n",
    "            initializer=np.zeros\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: np.array((n, d)), input values,\n",
    "                n - batch size, d - number of input units\n",
    "        :return: np.array((n, c)), output values,\n",
    "                n - batch size, c - number of output units\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        batch_size, input_units = inputs.shape\n",
    "        output_units, = self.output_shape\n",
    "        result = np.zeros((batch_size, output_units))\n",
    "        for res, inp in zip(result, inputs):\n",
    "            res[:] = self.biases + inp @ self.weights\n",
    "        return result\n",
    "        # your code here /\\\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        :param grad_outputs: np.array((n, c)), dLoss/dOutputs,\n",
    "                n - batch size, c - number of output units\n",
    "        :return: np.array((n, d)), dLoss/dInputs,\n",
    "                n - batch size, d - number of input units\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        batch_size, output_units = grad_outputs.shape\n",
    "        input_units, = self.input_shape\n",
    "        inputs = self.forward_inputs\n",
    "\n",
    "        # Don't forget to update current gradients:\n",
    "        # dLoss/dWeights\n",
    "        self.weights_grad[...] = sum(map(lambda x, y: np.matrix(x).T @ np.matrix(y), inputs, grad_outputs)) / batch_size\n",
    "        # dLoss/dBiases\n",
    "        self.biases_grad[...] = np.mean(grad_outputs, axis=0)\n",
    "        \n",
    "        result = np.zeros((batch_size, input_units))\n",
    "        for res, grad_out in zip(result, grad_outputs):\n",
    "            res[:] = grad_out @ self.weights.T\n",
    "            \n",
    "        return result\n",
    "        # your code here /\\\n",
    "\n",
    "\n",
    "# ============================ 2.2.1 Crossentropy ============================\n",
    "class CategoricalCrossentropy(Loss):\n",
    "    def __call__(self, y_gt, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_gt: np.array((n, d)), ground truth (correct) labels\n",
    "        :param y_pred: np.array((n, d)), estimated target values\n",
    "        :return: np.array((n,)), loss scalars for batch\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        batch_size, output_units = y_gt.shape\n",
    "        return np.sum(-y_gt * np.log(y_pred), axis=1)\n",
    "        # your code here /\\\n",
    "\n",
    "    def gradient(self, y_gt, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_gt: np.array((n, d)), ground truth (correct) labels\n",
    "        :param y_pred: np.array((n, d)), estimated target values\n",
    "        :return: np.array((n, d)), gradient loss to y_pred\n",
    "        \"\"\"\n",
    "        # your code here \\/\n",
    "        return -y_gt / y_pred\n",
    "        # your code here /\\\n",
    "\n",
    "\n",
    "# ================================ 2.3.1 SGD =================================\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr):\n",
    "        self._lr = lr\n",
    "\n",
    "    def get_parameter_updater(self, parameter_shape):\n",
    "        \"\"\"\n",
    "        :param parameter_shape: tuple, the shape of the associated parameter\n",
    "        :return: the updater function for that parameter\n",
    "        \"\"\"\n",
    "\n",
    "        def updater(parameter, parameter_grad):\n",
    "            \"\"\"\n",
    "            :param parameter: np.array, current parameter values\n",
    "            :param parameter_grad: np.array, current gradient, dLoss/dParam\n",
    "            :return: np.array, new parameter values\n",
    "            \"\"\"\n",
    "            # your code here \\/\n",
    "            assert parameter_shape == parameter.shape\n",
    "            assert parameter_shape == parameter_grad.shape\n",
    "            return parameter - self._lr * parameter_grad\n",
    "            # your code here /\\\n",
    "\n",
    "        return updater\n",
    "\n",
    "\n",
    "# ============================ 2.3.2 SGDMomentum =============================\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, lr, momentum=0.0):\n",
    "        self._lr = lr\n",
    "        self._momentum = momentum\n",
    "\n",
    "    def get_parameter_updater(self, parameter_shape):\n",
    "        \"\"\"\n",
    "        :param parameter_shape: tuple, the shape of the associated parameter\n",
    "        :return: the updater function for that parameter\n",
    "        \"\"\"\n",
    "\n",
    "        def updater(parameter, parameter_grad):\n",
    "            \"\"\"\n",
    "            :param parameter: np.array, current parameter values\n",
    "            :param parameter_grad: np.array, current gradient, dLoss/dParam\n",
    "            :return: np.array, new parameter values\n",
    "            \"\"\"\n",
    "            # your code here \\/\n",
    "            assert parameter_shape == parameter.shape\n",
    "            assert parameter_shape == parameter_grad.shape\n",
    "            assert parameter_shape == updater.inertia.shape\n",
    "\n",
    "            # Don't forget to update the current inertia tensor:\n",
    "            updater.inertia[...] = updater.inertia * self._momentum + self._lr * parameter_grad\n",
    "            return parameter - updater.inertia\n",
    "            # your code here /\\\n",
    "\n",
    "        updater.inertia = np.zeros(parameter_shape)\n",
    "        return updater\n",
    "\n",
    "\n",
    "# ======================= 2.4 Train and test on MNIST ========================\n",
    "def train_mnist_model(x_train, y_train, x_valid, y_valid):\n",
    "    # your code here \\/\n",
    "    # 1) Create a Model\n",
    "    model = Model(CategoricalCrossentropy(), SGDMomentum(0.1, 0.9))\n",
    "    # 2) Add layers to the model\n",
    "    #   (don't forget to specify the input shape for the first layer)\n",
    "    shape = np.shape(x_train)[1]\n",
    "#     first_dense = Dense(shape)\n",
    "#     first_dense.input_shape = shape\n",
    "    model.add(Dense(3 * np.shape(y_train)[1], input_shape=(shape,)))\n",
    "    model.add(Dense(3 * np.shape(y_train)[1]))\n",
    "    model.add(Softmax())\n",
    "    # 3) Train and validate the model using the provided data\n",
    "    model.fit(x_train, y_train, 32, 1,\n",
    "            shuffle=True, verbose=True,\n",
    "            x_valid=x_valid, y_valid=y_valid)\n",
    "    # your code here /\\\n",
    "    return model\n",
    "\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "b = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 10],\n",
       "       [12, 14]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b @ a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[..., :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2, 3]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix([1]).T @ np.matrix([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
